{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Faster_R_CNN_for_object_Detection",
      "provenance": [],
      "authorship_tag": "ABX9TyMimo0wsmxf+Zof8+pcu5Ta",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedElgabryi/faster-R-CNN/blob/master/Faster_R_CNN_for_object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DypqgFzeSekI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import theano\n",
        "import numpy\n",
        "from theano import tensor\n",
        "from blocks.model import Model\n",
        "from blocks.graph import ComputationGraph, apply_dropout\n",
        "from blocks.algorithms import StepClipping, GradientDescent, CompositeRule, RMSProp\n",
        "from blocks.filter import VariableFilter\n",
        "from blocks.extensions import FinishAfter, Timing, Printing, saveload\n",
        "from blocks.extensions.training import SharedVariableModifier\n",
        "from blocks.extensions.monitoring import DataStreamMonitoring, TrainingDataMonitoring\n",
        "from blocks.monitoring import aggregation\n",
        "from utils import get_metadata, get_stream, track_best, MainLoop\n",
        "from model import nn_fprop\n",
        "from config import config\n",
        "\n",
        "# Load config parameters\n",
        "locals().update(config)\n",
        "\n",
        "# DATA\n",
        "ix_to_char, char_to_ix, vocab_size = get_metadata(hdf5_file)\n",
        "train_stream = get_stream(hdf5_file, 'train', batch_size)\n",
        "dev_stream = get_stream(hdf5_file, 'dev', batch_size)\n",
        "\n",
        "\n",
        "# MODEL\n",
        "x = tensor.matrix('features', dtype='uint8')\n",
        "y = tensor.matrix('targets', dtype='uint8')\n",
        "y_hat, cost, cells = nn_fprop(x, y, vocab_size, hidden_size, num_layers, model)\n",
        "\n",
        "# COST\n",
        "cg = ComputationGraph(cost)\n",
        "\n",
        "if dropout > 0:\n",
        "    # Apply dropout only to the non-recurrent inputs (Zaremba et al. 2015)\n",
        "    inputs = VariableFilter(theano_name_regex=r'.*apply_input.*')(cg.variables)\n",
        "    cg = apply_dropout(cg, inputs, dropout)\n",
        "    cost = cg.outputs[0]\n",
        "\n",
        "# Learning algorithm\n",
        "step_rules = [RMSProp(learning_rate=learning_rate, decay_rate=decay_rate),\n",
        "              StepClipping(step_clipping)]\n",
        "algorithm = GradientDescent(cost=cost, parameters=cg.parameters,\n",
        "                            step_rule=CompositeRule(step_rules))\n",
        "\n",
        "# Extensions\n",
        "gradient_norm = aggregation.mean(algorithm.total_gradient_norm)\n",
        "step_norm = aggregation.mean(algorithm.total_step_norm)\n",
        "monitored_vars = [cost, gradient_norm, step_norm]\n",
        "\n",
        "dev_monitor = DataStreamMonitoring(variables=[cost], after_epoch=True,\n",
        "                                   before_first_epoch=True, data_stream=dev_stream, prefix=\"dev\")\n",
        "train_monitor = TrainingDataMonitoring(variables=monitored_vars, after_batch=True,\n",
        "                                       before_first_epoch=True, prefix='tra')\n",
        "\n",
        "extensions = [dev_monitor, train_monitor, Timing(), Printing(after_batch=True),\n",
        "              FinishAfter(after_n_epochs=nepochs),\n",
        "              saveload.Load(load_path),\n",
        "              saveload.Checkpoint(last_path),\n",
        "              ] + track_best('dev_cost', save_path)\n",
        "\n",
        "if learning_rate_decay not in (0, 1):\n",
        "    extensions.appenda(SharedVariableModifier(step_rules[0].learning_rate,\n",
        "                                             lambda n, lr: numpy.cast[theano.config.floatX](learning_rate_decay * lr), after_epoch=True, after_batch=False))\n",
        "\n",
        "print('number of parameters in the model: ' + str(tensor.sum([p.size for p in cg.parameters]).eval()))\n",
        "# Finally build the main loop and train the model\n",
        "main_loop = MainLoop(data_stream=train_stream, algorithm=algorithm,\n",
        "                     model=Model(cost), extensions=extensions)\n",
        "main_loop.run()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}